#+startup: overview
#+property: header-args:python :session *class*
#+title: kaggle playground 

In this project, I use a dataset announced in Kaggle for
classification of students into different groups based on different
predictors. You can access the dataset from this link.

My approach is to use multinomial logistic regression. We start by
importing required packages.

* Importing packages

#+begin_src python
  import numpy as np
  import seaborn as sns
  import pandas as pd
  from itertools import combinations
  import scipy.stats as st
  import matplotlib.pyplot as plt
  import statsmodels.api as sm
  import statsmodels.formula.api as smf
  from statsmodels.discrete.discrete_model import MNLogit
  import sklearn.neighbors as neib
  from sklearn.linear_model import LogisticRegression
  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
  from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
  from sklearn.metrics import confusion_matrix, accuracy_score
  qda=QDA()
  from sklearn.naive_bayes import GaussianNB
  nb=GaussianNB()
  lda=LDA(store_covariance=True)
  from sklearn.inspection import DecisionBoundaryDisplay as dbd
#+end_src

#+RESULTS:

* Importing data

#+begin_src python :results output
  df=pd.read_csv("train.csv")
  print("shape: ",df.shape)
  print("\ndtypes:\n",df.dtypes)
#+end_src

#+RESULTS:
#+begin_example
shape:  (76518, 38)

dtypes:
 id                                                  int64
Marital status                                      int64
Application mode                                    int64
Application order                                   int64
Course                                              int64
Daytime/evening attendance                          int64
Previous qualification                              int64
Previous qualification (grade)                    float64
Nacionality                                         int64
Mother's qualification                              int64
Father's qualification                              int64
Mother's occupation                                 int64
Father's occupation                                 int64
Admission grade                                   float64
Displaced                                           int64
Educational special needs                           int64
Debtor                                              int64
Tuition fees up to date                             int64
Gender                                              int64
Scholarship holder                                  int64
Age at enrollment                                   int64
International                                       int64
Curricular units 1st sem (credited)                 int64
Curricular units 1st sem (enrolled)                 int64
Curricular units 1st sem (evaluations)              int64
Curricular units 1st sem (approved)                 int64
Curricular units 1st sem (grade)                  float64
Curricular units 1st sem (without evaluations)      int64
Curricular units 2nd sem (credited)                 int64
Curricular units 2nd sem (enrolled)                 int64
Curricular units 2nd sem (evaluations)              int64
Curricular units 2nd sem (approved)                 int64
Curricular units 2nd sem (grade)                  float64
Curricular units 2nd sem (without evaluations)      int64
Unemployment rate                                 float64
Inflation rate                                    float64
GDP                                               float64
Target                                             object
dtype: object
#+end_example

The aim of classificatioin is to predict ~Target~

#+begin_src python
  set(df.Target)
#+end_src

#+RESULTS:
| Dropout | Graduate | Enrolled |

As we see, ~Target~ comes in three classes, namely Dropout, Graduate,
 and Enrolled.

Many of the predictors in out dataframe come in int format but
actually represent categorical data.

There are generally four types of data:
- Nominal (N)

- Ordinal (O)

- Interval (I)

- Ratio (R)

Using the column name, we assign a type to each predictor written in
  side parenthesis:

- (N)Marital status                                      int64
- (N)Application mode                                    int64
- (N)Application order                                   int64
- (N)Course                                              int64
- (N,B)Daytime/evening attendance                          int64
- (N)Previous qualification                              int64
- (I)Previous qualification (grade)                    float64
- (N)Nacionality                                         int64
- (N)Mother's qualification                              int64
- (N)Father's qualification                              int64
- (N)Mother's occupation                                 int64
- (N)Father's occupation                                 int64
- (R)Admission grade                                   float64
- (N,B)Displaced                                           int64
- (N,B)Educational special needs                           int64
- (N,B)Debtor                                              int64
- (N,B)Tuition fees up to date                             int64
- (N,B)Gender                                              int64
- (N,B)Scholarship holder                                  int64
- (R)Age at enrollment                                   int64
- (N,B)International                                       int64
- (R)Curricular units 1st sem (credited)                 int64
- (R)Curricular units 1st sem (enrolled)                 int64
- (R)Curricular units 1st sem (evaluations)              int64
- (R)Curricular units 1st sem (approved)                 int64
- (R)Curricular units 1st sem (grade)                  float64
- (R)Curricular units 1st sem (without evaluations)      int64
- (R)Curricular units 2nd sem (credited)                 int64
- (R)Curricular units 2nd sem (enrolled)                 int64
- (R)Curricular units 2nd sem (evaluations)              int64
- (R)Curricular units 2nd sem (approved)                 int64
- (R)Curricular units 2nd sem (grade)                  float64
- (R)Curricular units 2nd sem (without evaluations)      int64
- (R)Unemployment rate                                 float64
- (R)Inflation rate                                    float64
- (R)GDP                                               float64
- (N)Target                                             object
- dtype: object
#+end_example

For ratio level data we can use boxplots to see their distribution:

#+begin_src python :results file
  df.iloc[:,22:34].plot.box(label=df.columns[22:34],figsize=(10,10))
  plt.xticks(rotation=90)
  plt.tight_layout()
  file_name="images/box_curricular"
  plt.savefig(file_name)
  file_name
#+end_src

#+RESULTS:
[[
file:images/box_curricular.png]]

** Changing data types

I change the type of data to categorical data for those that come in
integer format but really represent categories:

#+begin_src python
  df_c=df.copy()
  col_list=list(range(1,7))+list(range(8,13))+list(range(14,20))+[21]+[37]
  df_c[df_c.columns[col_list]]=df[df.columns[col_list]].astype("category")
#+end_src

#+RESULTS:

* Describing data

We have different options for doing classification. These options
depend on how our data is spread for different predictors. So,
we plot bar chart of our categorical data:

** Count plots

#+begin_src python
  for i in col_list:
      sns.countplot(data=df_c[df_c.columns[i]])
      plt.savefig(f"images/countplot_{i}")
      plt.close("all")
#+end_src

#+RESULTS:

View countplots:
#+begin_src python :results list
  [f"[[file:images/countplot_{i}.png][{df_c.columns[i]}]]" for i in col_list]
#+end_src

#+RESULTS:
- [[file:images/countplot_1.png][Marital status]]
- [[file:images/countplot_2.png][Application mode]]
- [[file:images/countplot_3.png][Application order]]
- [[file:images/countplot_4.png][Course]]
- [[file:images/countplot_5.png][Daytime/evening attendance]]
- [[file:images/countplot_6.png][Previous qualification]]
- [[file:images/countplot_8.png][Nacionality]]
- [[file:images/countplot_9.png][Mother's qualification]]
- [[file:images/countplot_10.png][Father's qualification]]
- [[file:images/countplot_11.png][Mother's occupation]]
- [[file:images/countplot_12.png][Father's occupation]]
- [[file:images/countplot_14.png][Displaced]]
- [[file:images/countplot_15.png][Educational special needs]]
- [[file:images/countplot_16.png][Debtor]]
- [[file:images/countplot_17.png][Tuition fees up to date]]
- [[file:images/countplot_18.png][Gender]]
- [[file:images/countplot_19.png][Scholarship holder]]
- [[file:images/countplot_21.png][International]]
- [[file:images/countplot_37.png][Target]]


* Regression with MNLogit

We use MNLogit method drom Statsmodels since this package gives us
inference. Looking into the distribution of categories, we realize
that the distribution of frequency of categories has significant
outliers. These make optimization of multinomial logistic classifier
impossible. To work around this problem, in our first approach, we
replace categorical data with their frequencies. This is called
frequency coding. 

** Transforming input data using frequency coding

#+begin_src python
  freq_list=[df[df.columns[col_list[i]]].value_counts(normalize=True) for i in range(len(col_list))]
  map_list=[df[df.columns[col_list[i]]].map(freq_list[i]) for i in range(len(freq_list))]
#+end_src

#+RESULTS:

#+begin_src python
  df_map=df.drop(columns=df.columns[col_list])
  [df_map.insert(0,df.columns[col_list[i]],map_list[i]) for i in range(len(map_list))]
  pass
#+end_src

#+RESULTS:

** MNLogit

Now we use mapped data with MNLogit method:

#+begin_src python
  X_map=df_map.drop(columns=["id","Target"])
  X_map=sm.add_constant(X_map)
  mod_map=MNLogit(df_map.Target,X_map).fit()
#+end_src

#+RESULTS:

Confusion table:

#+begin_src python
  ct=mod_map.pred_table()
  ct
#+end_src

#+RESULTS:
| 8267 |  1714 |  4959 |
| 3054 | 20595 |  1647 |
| 2008 |   739 | 33535 |

Accuracy:

#+begin_src python
  np.sum(ct.diagonal())/np.sum(ct)
#+end_src

#+RESULTS:
: 0.81545518701482

The summary of model with all its parameters is large and saved in a
text file:

#+begin_src python :results file
  file_name="summary_logit.txt"
  with open(file_name,"w") as file:
      file.write(str(mod_map.summary()))
  file_name
#+end_src

#+RESULTS:
[[
file:summary_logit.txt]]

Looking into the p value of coefficients, we can easily say that
~Nacionality~ and ~Internation~ predictors are not related to the
target. As a test, we remove them from our input data and train the
model again:

#+begin_src python
  X_map_r=X_map.drop(columns=["International","Nacionality" ])
  mod_map_r=MNLogit(df_map.Target,X_map_r).fit()
#+end_src

#+RESULTS:

#+begin_src python
  ct=mod_map_r.pred_table()
  ct
#+end_src

#+RESULTS:
| 8265 |  1707 |  4968 |
| 3048 | 20600 |  1648 |
| 2015 |   739 | 33528 |

#+begin_src python
  np.sum(ct.diagonal())/np.sum(ct)
#+end_src

#+RESULTS:
: 0.8154029117331869

We see that the accuracy of the model did not change.

* Refining data with categories
In our next approach, we keep the categorical data as categorical, but
we group categories that has frequency of less than 1% into a new
category called ~other~. Using this trick enables us to train the
model with categorical data:

#+begin_src python
  freq_list_c=[df_c[df_c.columns[col_list[i]]].value_counts(normalize=True) for i in range(len(col_list))]
  ref=[df_c[df_c.columns[col_list[i]]].apply(lambda x: "other" if freq_list_c[i].at[x]<.01 else x) for i in range(len(col_list))]
  df_ref=pd.DataFrame(ref).T
  df_ref=df_ref.astype("category")
#+end_src

#+RESULTS:

#+begin_src python
  df_new=df_ref.join(df_c.drop(columns=df_c.select_dtypes("category")))
#+end_src

#+RESULTS:
: None

Training the model:

#+begin_src python
  X=df_new.drop(columns=["id","Target"])
  X=pd.get_dummies(X)
  X=sm.add_constant(X)
  mod_new=MNLogit(df_new.Target,X).fit()
#+end_src

#+RESULTS:

#+begin_src python
  pt=mod_new.pred_table()
  np.sum(np.diagonal(pt))/np.sum(pt)
#+end_src

#+RESULTS:
: 0.8242505031495857

We see a little bit of improvement over previous model.

#+begin_src python
  file_name="summary_new.txt"
  with open(file_name,"w") as file:
      file.write(str(mod_new.summary()))
  file_name
#+end_src

#+RESULTS:
: summary_new.txt

* Conclusion

- Using frequency coding we managed to get inference about
  classification of our data.
- Using grouping classes, we managed to make optimization of
  multinomial logistic classifier stable and improve the accuracy.
- Tuning the model can be further improved by making inference from
  the first model
