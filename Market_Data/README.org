#+startup: overview
#+property: header-args:python :session *Exer_chap_4* :results silent
#+title: Throwing models on market data!

~Weekly~ is part of the ISLP library. This dataset consists of percentage returns for the S&P 500 stock index over 1089 weeks, from 1990 until the end of 2010.

For each date, the percentage returns for each of the five previous trading weeks are recorded, Lag1 through Lag5. Volume (the number of shares traded on the previous week, in billions), Today (the percentage return on the week in question) and Direction (whether the market was Up or Down on this date) are also recorded.

Market data are notoriously difficult to predict. I will test different models both parametric and non parametric on this dataset.

* Import
#+begin_src python
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import ISLP as bk
  import statsmodels.api as sm
  from sklearn import metrics
  from ISLP.models import (ModelSpec as MS, summarize, poly)
  from ISLP import confusion_table
  from sklearn.discriminant_analysis import \
      (LinearDiscriminantAnalysis as LDA,
       QuadraticDiscriminantAnalysis as QDA)
  from sklearn.naive_bayes import GaussianNB
  from sklearn.neighbors import KNeighborsClassifier
  from sklearn.preprocessing import StandardScaler
  from sklearn.model_selection import \
      (cross_validate ,
       KFold ,
       ShuffleSplit)
  import seaborn as sns
  from sklearn.metrics import roc_auc_score
  from sklearn.metrics import confusion_matrix
  from ISLP.models import sklearn_sm
#+end_src

* Loading data

#+begin_src python :results replace output
  df=bk.load_data("Weekly")
  df.info()
#+end_src

#+begin_example
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1089 entries, 0 to 1088
Data columns (total 9 columns):
 #   Column     Non-Null Count  Dtype   
---  ------     --------------  -----   
 0   Year       1089 non-null   int64   
 1   Lag1       1089 non-null   float64 
 2   Lag2       1089 non-null   float64 
 3   Lag3       1089 non-null   float64 
 4   Lag4       1089 non-null   float64 
 5   Lag5       1089 non-null   float64 
 6   Volume     1089 non-null   float64 
 7   Today      1089 non-null   float64 
 8   Direction  1089 non-null   category
dtypes: category(1), float64(7), int64(1)
memory usage: 69.4 KB
#+end_example

#+begin_src python :results replace value 
  df.Year.value_counts().sort_index()
#+end_src

#+begin_example
Year
1990    47
1991    52
1992    52
1993    52
1994    52
1995    52
1996    53
1997    52
1998    52
1999    52
2000    52
2001    52
2002    52
2003    52
2004    52
2005    52
2006    52
2007    53
2008    52
2009    52
2010    52
Name: count, dtype: int64
#+end_example

* Visualizing the data

#+begin_src python :results replace file :file images/pairplot.png
  fig, ax = plt.subplots( figsize=(10,10), layout='tight') 
  sns.pairplot(df, hue=df.columns[-1])
  plt.savefig("images/pairplot.png")
#+end_src

[[file:images/pairplot.png]]

#+begin_src python :results replace file :file images/corr_matrix.png
  # fig, ax = plt.subplots(figsize=(5,5), layout='tight')
  plt.close("all")
  sns.heatmap(df.iloc[:,:-1].corr())
  # fig.savefig("images/corr_matrix.png")
  plt.savefig("images/corr_matrix.png")
#+end_src

[[file:images/corr_matrix.png]]
* Scaling data
Scaling is preformed for ~Lag~ columns using ~Lag1~ as the base column. ~Volume~ is scaled separately.

#+begin_src python
  scaler = StandardScaler()
  scaler.fit(df[["Lag1"]])
  for col in ["Lag1", "Lag2", "Lag3", "Lag4", "Lag5"]:
      df[col] = (df[col]-scaler.mean_)/scaler.scale_
  df["Volume"] = scaler.fit_transform(df[["Volume"]])
#+end_src

* A null mode
#+begin_src python :results replace value 
  res = confusion_matrix(df.Direction=="Up", [True]*df.shape[0])
  res.diagonal().sum()/res.sum()
#+end_src

: 0.5555555555555556

A null model that always predicts ~Up~ direction has an accuracy of 0.55.
* Logistic Regression
** with one predictor

#+begin_src python :results replace output
  # defining a scores based on roc_auc
  def glm_auc_score(estimator, X_test, y_test):
    y_prob = estimator.predict(X_test)
    return roc_auc_score(y_test, y_prob)
  # defining a scores using accuracy of classification
  def glm_table_score(estimator, X_test, y_test):
    res = estimator.predict(X_test) > .5
    table = confusion_matrix(y_test, res)
    accuracy = table.diagonal().sum()/table.sum()
    return accuracy

  # doing cross validation for different models
  for col in df.columns[:-2]:
      design = MS([col])
      X = design.fit_transform(df)
      y = df.Direction == "Up"
      model = sklearn_sm(sm.GLM, model_args={'family':sm.families.Binomial()})
      cv_models = cross_validate(model, X, y, cv=10, scoring=glm_auc_score)
      print("Using only", col, "Score:", round(cv_models["test_score"].mean(), 2)
#+end_src

Scores using ~glm_auc_score~
: Using only Year Score: 0.51
: Using only Lag1 Score: 0.54
: Using only Lag2 Score: 0.52
: Using only Lag3 Score: 0.51
: Using only Lag4 Score: 0.51
: Using only Lag5 Score: 0.53
: Using only Volume Score: 0.50

Scores using ~glm_table_score~
: Using only Year Score: 0.55
: Using only Lag1 Score: 0.55
: Using only Lag2 Score: 0.55
: Using only Lag3 Score: 0.55
: Using only Lag4 Score: 0.55
: Using only Lag5 Score: 0.55
: Using only Volume Score: 0.54

** with all predictors
#+begin_src python :results replace output
  design = MS(["Lag1","Lag2", "Lag3","Lag4","Lag5", "Volume"])
  X = design.fit_transform(df)
  model = sklearn_sm(sm.GLM, model_args={'family':sm.families.Binomial()})
  cv_models = cross_validate(model, X, y, cv=10, scoring=glm_table_score)
  print("Score:", round(cv_models["test_score"].mean(), 2))
#+end_src

: Score: 0.54

Scores using ~glm_table_score~
: Score: 0.54

Scores using ~glm_auc_score~
: Score: 0.53

To see the p-values for a model with all predictors:

#+begin_src python :results replace value 
  model = sm.Logit(y, X).fit()
  model.pvalues
#+end_src

: intercept    0.000243
: Lag1         0.118144
: Lag2         0.029601
: Lag3         0.546924
: Lag4         0.293653
: Lag5         0.583348
: Volume       0.537675
: dtype: float64

The ~Lag2~ shows a significant pvalue but its effect is not strong enough for prediction.

** with added predictors
I add second degree of ~Lag~ predictors to see if this has any effect on the model's score.

#+begin_src python :results replace output
  design=MS([ poly("Lag1",2), poly("Lag2",2), poly("Lag3",2),  poly("Lag4",2),  poly("Lag5",2), "Volume"])
  X=design.fit_transform(df)
  model = sklearn_sm(sm.GLM, model_args={'family':sm.families.Binomial()})
  cv_models = cross_validate(model, X, y, cv=10, scoring=glm_table_score)
  print("Score:", round(cv_models["test_score"].mean(), 3))
#+end_src

: Score: 0.534

* Linear Discriminant Analysis (LDA)

#+begin_src python :results replace value 
  lda=LDA(store_covariance=True)
  # X=X.drop(columns=["intercept"])
  cv_models = cross_validate(lda, X, y, cv=10)
  cv_models["test_score"].mean()
#+end_src

: 0.5389568467550119

* Quadratic Discriminant Analysis (QDA)


#+begin_src python :results replace value 
  qda=QDA(store_covariance=True)
  cv_models = cross_validate(qda, X, y, cv=10)
  cv_models["test_score"].mean()
#+end_src

: 0.5380818892286783

* Naive Bayes (NB)

#+begin_src python :results replace value 
  NB = GaussianNB()
  cv_models = cross_validate(NB, X, y, cv=10)
  cv_models["test_score"].mean()
#+end_src

: 0.5307084607543323

* K-nearest neighbors (KN)

I test K-nearest neighbors using cross validation for up to 20 neighbors: 
#+begin_src python :results replace output
  for neighbor in range(1,21):
        knn=KNeighborsClassifier(n_neighbors=neighbor)
        cv_models = cross_validate(knn, X, y, cv=5)
        print("neighbors:", neighbor, ", CV score:", round(cv_models["test_score"].mean(),3))

#+end_src

#+begin_example
neighbors: 1 , CV score: 0.486
neighbors: 2 , CV score: 0.481
neighbors: 3 , CV score: 0.514
neighbors: 4 , CV score: 0.488
neighbors: 5 , CV score: 0.521
neighbors: 6 , CV score: 0.512
neighbors: 7 , CV score: 0.533
neighbors: 8 , CV score: 0.514
neighbors: 9 , CV score: 0.522
neighbors: 10 , CV score: 0.519
neighbors: 11 , CV score: 0.519
neighbors: 12 , CV score: 0.512
neighbors: 13 , CV score: 0.527
neighbors: 14 , CV score: 0.521
neighbors: 15 , CV score: 0.524
neighbors: 16 , CV score: 0.52
neighbors: 17 , CV score: 0.525
neighbors: 18 , CV score: 0.515
neighbors: 19 , CV score: 0.533
neighbors: 20 , CV score: 0.515
#+end_example

The best result is achieved for K=7 but for K>4 the variance is small. 

* Conclusion

All models I tried could not beat the null model. Stating the obvious, market data remain unpredictable. In other words, market is efficient! 
